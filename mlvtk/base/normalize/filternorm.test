import tensorflow as tf
from typing import List, TypeVar, Union
from tqdm.auto import tqdm
import numpy as np
import pandas as pd
from memory_profiler import profile
import dask.array as da
import dask.multiprocessing
dask.config.set(scheduler='processes')
import numcodecs
import zarr
import pathlib


T = TypeVar("T")
LLNP = Union[List[np.float32], List[List[np.float32]]]  # type: ignore


def _linspace_dispatcher(j, jdiff, jsize):
    return np.linspace(j.min() - jdiff, j.max() + jdiff, num=jsize, dtype=np.float32)  # type: ignore


def old_reshape(values, weights):
    s = 0
    for w in weights:
        yield np.reshape(values[s : s + w.size], w.shape)
        s += w.size

def _reshape(values, weights):
    s = 0
    nw = []
    for w in weights:
        nw.append(np.reshape(values[s : s + w.size], w.shape))
        s += w.size
    return nw
def _normalize_filter(fw: np.ndarray) -> np.ndarray:
    f, w = fw
    return f * np.linalg.norm(w.flatten()) / (np.linalg.norm(f.flatten()) + 1e-10)


def _evaluate(model, weights, data, filters):
    new_weights = _reshape(filters, weights) #np.add(weights, filters)
    model.set_weights(new_weights)
    if not isinstance(data, tf.data.Dataset):
        return model.evaluate(
            x=data[0], y=data[1], use_multiprocessing=True, verbose=0, return_dict=True
        ).get("loss")

    return model.evaluate(
        data, use_multiprocessing=True, verbose=0, return_dict=True
    ).get("loss")

def normalizer(
    model,
    traj_calc=None,
    alphas_size: int = 35,
    betas_size: int = 35,
    extension: T = 1,
    quiet=False,
    **kwargs,
):

    def mini_reshape(values, interval):
        return da.reshape(values[interval[0]:interval[1]], weights[interval[2]].shape).compute()

    weights = model.get_weights()

    waits = [w.size for w in weights]

    waits = np.cumsum(waits)

    intervals = [(0, waits[0], 0), *[(waits[i], e, i+1) for i, e in enumerate(waits[1:])]]


    for key in kwargs:
        assert key in [
            "alphas",
            "betas",
            "xdir",
            "ydir",
            "pca_dirs",
        ], f"{key} is not a\
        valid kwarg"

    assert (
        traj_calc is not None or kwargs.get("xdir") is not None
    ), "Need traj_calc or xdir/ydir"

    if traj_calc:
        xdir, ydir = traj_calc.xdir, traj_calc.ydir
        pca_dirs = [da.from_array(arr) for arr in traj_calc.pca_dirs]

    else:
        xdir, ydir = kwargs["xdir"], kwargs["ydir"]
        if "pca_dirs" in kwargs:
            pca_dirs =  kwargs["pca_dirs"]
        else:
            pca_dirs = None

    if np.size(xdir) == np.size(ydir) and np.size(xdir) <= 1:  # type: ignore
        alphas = np.linspace(-5, 5, num=alphas_size)  # type: ignore
        betas = np.linspace(-5, 5, num=betas_size)  # type: ignore

    elif extension == "std":
        xdiff = np.std(xdir)
        ydiff = np.std(ydir)

    elif extension == 1:
        xdiff = ydiff = 1

    else:
        xdiff, ydiff = extension

    if np.ndim(xdir) > 1:
        concat_xdir = np.concatenate(xdir)  # type: ignore
        concat_ydir = np.concatenate(ydir)  # type: ignore
        alphas = _linspace_dispatcher(concat_xdir, xdiff, alphas_size)  # type: ignore
        betas = _linspace_dispatcher(concat_ydir, ydiff, betas_size)  # type: ignore
        coordinate_list = [np.column_stack([a, b]) for a, b in zip(xdir, ydir)]

    else:
        alphas = _linspace_dispatcher(xdir, xdiff, alphas_size)  # type: ignore
        betas = _linspace_dispatcher(ydir, ydiff, betas_size)  # type: ignore
        coordinate_list = np.column_stack([xdir, ydir])

    # For readability, make local vars
    alph = alphas[None, :, None]  # [[[`val`], [`val`],...]]
    bet = betas[:, None, None]  # [[[`val`]], [[`val`]]...]
    bet_zeros = np.zeros_like(bet)
    alph_zeros = np.zeros_like(alph)

    # in order to construct cartesian product:
    # take advantage of broadcasting with tf.add.
    # example:::
    # alphas.shape -> (50,)
    # betas.shape -> (50,)
    # alph.shape/alph_zeros.shape -> (1, 50, 1)
    # bet.shape/bet_zeros.shape -> (50, 1, 1)
    #
    # By doing sum_alph = tf.add(alph, bet_zeros) and sum_bet = tf.add(bet, alph_zeros)
    # we get two tensors of shape (50, 50, 1).
    # Then we tf.concat([sum_alph, sum_bet], axis=2).
    # By doing it along axis=2, we get pairs of values
    # in shape (50, 50, 2). Finally, we use tf.reshape
    # to get final cartesian product in result.shape = (2500, 2),
    # so then result[i] == [`val_i_0`, `val_i_1`]
    # adapted from https://stackoverflow.com/a/50195230
    alphas_betas_list = np.reshape(
        np.concatenate([alph + bet_zeros, alph_zeros + bet], axis=2),  # type: ignore
        (alphas.shape[0] * betas.shape[0], 2),
    )
    # alphas_betas_list = np.column_stack([alphas, betas])

    if pca_dirs is None:
        # gamma <- delta
        # delta <- eta
        gamma = da.array([_normalize_filter((da.random.normal(size=ww.size), ww)) for ww in weights])  # type: ignore
        delta = da.array([_normalize_filter((da.random.normal(size=ww.size), ww)) for ww in weights])  # type: ignore

    else:
        pca_gamma, pca_delta = pca_dirs
        gamma = []
        delta = []
        i = 0
        for l in weights:
            gamma.append(_normalize_filter((pca_gamma[i : i + l.size], l)))
            delta.append(_normalize_filter((pca_delta[i : i + l.size], l)))
            i += l.size

    batch_norm_filter = filter(
        lambda layer: "batch_normalization" in layer.name, model.layers
    )
    if batch_norm_filter:
        for layer in batch_norm_filter:
            i = model.layers.index(layer)
            gamma[i] = da.zeros((gamma[i].shape))  # type: ignore
            delta[i] = da.zeros((delta[i].shape))  # type: ignore

    w_concat = da.concatenate([w.flatten() for w in weights])
    g_concat = da.concatenate(gamma)
    d_concat = da.concatenate(delta)
    if not pathlib.Path('filters'):
        pathlib.Path('filters/nfilters').mkdir(parents=True)
       # pathlib.Path('filters/ofilters').mkdir(parents=True)

    print("rechunk")
    normalized_filters = da.stack(
        [w_concat + (g_concat * a + d_concat * b) for (a, b) in alphas_betas_list]
    ).rechunk(balance=True).to_zarr('filters/nfilters.zarr', overwrite=True)

    non_eager_model = model._new_model()

    if np.ndim(coordinate_list) > 2:

        optimizer_filters = [
            da.stack(
                [w_concat + (g_concat * a + d_concat * b) for (a, b) in coord_list]
            )
            for coord_list in coordinate_list
        ]

        optimizer_losses = []

        for mod in tqdm(optimizer_filters, desc="optimizer path", disable=quiet):
            t = []
            for filt in tqdm(mod, desc="filter", disable=quiet):
                t.append(_evaluate(filt))
            optimizer_losses.append(t)

    else:
        optimizer_filters = da.stack(
            [w_concat + (g_concat * a + d_concat * b) for (a, b) in coordinate_list]
        )
        #optimizer_losses = np.zeros((np.size(xdir)))  # type: ignore
        optimizer_losses = []

        print("opt")
        for i, filt in enumerate(tqdm(optimizer_filters, desc="filter", disable=quiet)):
            optimizer_losses.append(_evaluate(non_eager_model, weights, model.validation_data, filt.compute()))

    del optimizer_filters

    #filter_losses = np.zeros((alphas_size * betas_size))
    filter_losses = []
    z = zarr.open('filters/nfilters.zarr', mode='r')
    for i, filt in enumerate(tqdm(z, desc="norm_filters", disable=quiet)):
        filter_losses.append(_evaluate(non_eager_model, weights, model.validation_data,z))
    
    
    del normalized_filters
    return {
        "surface": pd.DataFrame(
            data=np.reshape(filter_losses, (np.size(alphas), np.size(betas))),
            index=alphas,
            columns=betas,
        ),
        "optimizer": (xdir, ydir, optimizer_losses),
    }
